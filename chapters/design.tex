\chapter{Design}\label{chapter:design}

\section{The Algorithm: An Abstract Description}\label{sec:abstract_algo}

\subsection{Local Discreteness}\label{sec:local_discreteness}

Requiring a tested translator to satisfy local discreteness is imperative to verify whether $P_X \equiv E_{X \rightarrow
Y}(P_X)$. This is because, if this requirement weren't assumed, a verifier would either have to know the result of
comparing every possible program of $X$ to every possible program of $Y$ in advance (which is obvious nonsense as both
of these sets are infinitely large) or be a sort of theoretical abstract tautology, in other words an algorithm that is
not actually implementable. This argument simultaneously shows that every implementation of a binary translator
\textbf{must necessarily} be locally discrete for the same reasons.

However, when we use the definition of program equivalence to transform the problem into $P_X(S) = [E_{X \rightarrow
Y}(P_X)](S)$, the global comparison of result states becomes possible again. While this removes the theoretical
necessity for the local discreteness assumption, it is still practically useful, if not essential. An outcome of
comparing result states of full program executions would be a statement like "The program's translation is erroneous".
This is too general of an assertion to be useful to developers. When we instead use local discreteness and check for all
intermediate states whether $S_i = S^E_i$ (where $S_i = x_{i-1}(S_{i-1})$ and $S^E_i = E_{X \rightarrow
Y}(x_{i-1})(S^E_{i-1})$ with $S_1 = S^E_1$ an arbitrary initial state), we can generate much more detailed information,
such as "Instruction $x_i$ is translated incorrectly because $S_i \neq S^E_i$". This is exactly the type of feedback
that Focaccia seeks to provide.

Additionally, it lets the algorithm recover from errors: Even if an instruction $x_i$ turns out to be implemented
incorrectly, thus producing an incorrect state $S^E_{i+1}$, that state is only incorrect from the viewpoint of program
semantics. The verifier, however, can still treat it as a valid input state to all subsequent instructions $x_{j > i}$
because the translation $E_{X \rightarrow Y}(x_j)$ is required to be \textit{locally correct} for a locally discrete
translator, that is, it must work correctly on \textbf{any} program state.

\subsection{Comparing Program States}\label{sec:comparison}

A naive verification algorithm works as follows: We run $P_X$ on one initial state $S$ and the translation $E_{X
\rightarrow Y}(P_X)$ on the same initial state, during which we record the respective intermediate states $S_i$ and
$S^E_i$. For each pair of states, test whether $S_i = S^E_i$. If this equality does \textbf{not} hold, then the
translator's implementation of $x_i$, meaning the translation $x_i \mapsto (y_j)$, is faulty.

The intuitive way of implementing the equality operator on program states ($S = S'$) is by comparing register- and
memory content. These values are what constitute the state of a program, and they are numeric values with a canonical
condition for equality. However, as section \ref{sec:intro:focaccia} indicated, comparing program states is more complex
in reality. That is because the starting states for the execution of programs are not always equal: the previous
assumption that $S_1 = S^E_1$ is not necessarily true. In fact, it is most commonly false. Possible contributing factors
(subsequently collectivized as a general \textit{difference in environment}) include:

\begin{itemize}
    \item Different initial stack pointers.
    \item Different addresses of heap allocations.
    \item Different environment- and auxiliary vectors. The latter is particularly interesting. It turns out that
        the auxiliary vector provided by QEMU, for example, routinely differs from the one provided by the operating
        system. See section \ref{sec:auxv} for more details.
\end{itemize}

Therefore, instead of comparing $P_X(S) = P_Y(S')$, which does \textbf{not} imply $P_X \equiv P_Y$ if $S \neq S'$, the
comparison must take into account the initial difference $\Delta_S = S - S'$ and establish a \textbf{state equivalence}
$P_X(S) \equiv_{\Delta_S} P_Y(S') \implies P_X \equiv P_Y$ with respect to it. This is the chief nontriviality that
Focaccia's algorithm solves.

The way we calculate this equivalence is by re-introducing information about the guest instructions $x_i$ to the
algorithm which, in order to simplify the problem, we have discarded when we transformed the central question from $x
\equiv E_{X \rightarrow Y}(x)$ to $x(S) = [E_{X \rightarrow Y}(x)](S)$. The new algorithm works as follows: Instead of
running guest program and translation in parallel and comparing their intermediate states, only run the translation
$E_{X \rightarrow Y}(P_X)$ on a start state $S^E_1$, thereby obtaining the translation's intermediate states $S^E_i$.
Then, for each $S^E_i$, use the guest instruction $x_i$ to calculate a corresponding \textbf{expected state} $S_{i+1} =
x_i(S^E_i)$. These represent truth states that would result from executing $x_i$ on $S^E_i$ if $x_i$ was implemented
correctly. Finally, compare the expected state to the actual translation state: $S^E_{i+1} = S_{i+1}$. Again, this works
because it does not matter to the verifier whether $S^E_{i+1}$ is a \textit{correct} state with regards to whole-program
semantics.

Decomposing the pre-translation program $(x_i)$ into its instructions and applying them selectively to the synthetic
test states as opposed to executing it natively on a semi-random starting state thus allows us to use the same starting
state for both the translation and the truth program at each instruction, eliminating $\Delta_S$. This enables the
desired equality comparison $x_i(S_i) = E_{X \rightarrow Y}(x_i)(S_i)$. The drawback of this approach is that we now
require an additional piece of information: We need to know $x_i$.

Not only do we need to know what every $x_i$ is, but we also demand a way of applying it individually to an arbitrary
program state which we determine, or, more precisely, which is being determined by the translation's execution. One
approach would be to run a program that sets up the correct machine state, then runs the instruction in question, and
finally reads the state back. We chose a different path: We use symbolic execution tools to translate instructions into
equations, which we then manually apply to the states we want to test.

\section{Symbolic Execution}\label{sec:symb_exec}

Symbolic execution is a technique in which programs are executed abstractly with symbolic values~\cite{Steinhöfel2022}.
Certain values at any point in the program are replaced by symbols that represent multiple or all possible values.
Symbolic execution then tracks those symbols' evolution throughout the program's execution.
\lstlistingname~\ref{fig:symbexec_example_listing} shows a code example. In a usual \textit{concrete} execution,
\texttt{a} is exactly one value and subsequent code uses that value to perform calculations. If, for example, we set
\texttt{a = 4}, the program calculates the following values: \texttt{a = 4}, \texttt{b = -1}, \texttt{c = -3}. If, on
the other hand, in a symbolic execution of the same code, \texttt{a} is set to a generic symbol \texttt{a = $\alpha$},
the symbolic execution engine produces the following output: \texttt{a = $\alpha$}, \texttt{b = $\alpha$ - 5}, \texttt{c
= ((($\alpha$ - 5) \% 3) == 0) ? ($\alpha$ + 10) : (-3)}. The entire program is expressed depending on the generic
symbol $\alpha$.

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Python]
        a = ?
        b = a - 5
        c = a + 10 if (b % 3) == 0 else -3
    \end{lstlisting}
    \end{tabular}
    \caption[Symbolic execution example]{Symbolic Execution Sample Code}\label{fig:symbexec_example_listing}
\end{figure}

% TODO: Can I use citations from Steinhöfel2022 to cite every single of the following use cases or is the single
% reference to the Steinhöfel2022 chapter sufficient?

This technique is often used to reverse-engineer program inputs, to implement symbolic debugging, or to test programs
either by proving properties of the resulting equations (e.g.\ when is \texttt{c == 12}?) or by generating test cases
from the equation. We use symbolic execution tools for machine code to translate instructions into manipulable
functions/equations which we can then apply to program states via an interpreter. It turns out, that, if one executes an
instruction via a symbolic execution engine on an input state which is not only partly but entirely symbolic, the
resulting equation is a full representation of the transformation which is applied to a program state by that
instruction. This is an uncommonly seen use of symbolic execution as inputs to a program are usually symbolized
selectively, otherwise the branching complexity of full programs quickly leads to an exponential inflation of the
equations' size as well as the number of explored paths. This is not a problem if we only trace a single instruction at
any time: no symbolic branching conditions are ever propagated through multiple instructions. Overall, only one path
through the program is considered and the effort thus remains linear.

Formally, we denote this use of symbolic execution as a map $G$ from an instruction $x \in X$ to a symbolic entity
$\sigma$ which is composed from a symbolic alphabet $\Sigma$ and is itself a function of program states: $G_{X
\rightarrow \Sigma}: x \mapsto \sigma$ with $x(S) = \sigma(S)$ where $G$ is called the \textbf{symbolic expression
generator} for $X$. The perceptive reader will have noticed that this translation resembles a locally discrete binary
translator (though a generalized version whose target language is not specifically an \ac{ISA}, but an abstract symbolic
language), with the addition of a \textit{correctness condition}. This is in fact the case. Essentially, Focaccia relies
on the existence of one correct binary translator $E_{X \rightarrow \Sigma}$ which translates the guest \ac{ISA} $X$ to
a symbolic language $\Sigma$. See section~\ref{sec:symb_exec_backend} for further discussion of problems that this
approach bears.

\subsection{Obtaining an Instruction}

In the first prototype, we used a disassembly framework to load a binary and disassemble it in its entirety. This was
slow and produced many unnecessary computations (disassembling instructions that are never touched), so we started
disassembling instructions on demand, i.e., at each program counter, only read the next instruction. This works for
statically linked binaries as all information is in one file and can be loaded from there. In the third iteration, in
order to support dynamically linked programs and even \ac{JIT} compiled code, we omit loading the binary entirely and
instead read the current instruction directly from the running program's memory; this is usually a concrete execution
running alongside Focaccia.

\subsection{A Symbolic Representation}

The component concerned with everything symbolic execution related is the \textbf{symbolic expression generator}. Its
task is to translate instructions into corresponding symbolic representations which capture the instructions' semantics.
Figures~\ref{fig:symb_equation_mov} and~\ref{fig:symb_equation_add} show examples of symbolic equations generated from a
\texttt{MOV} instruction and an \texttt{ADD} instruction, respectively. Section~\ref{sec:symb_expr_impl} details how
this abstract expression generator is implemented in Focaccia and discusses the challenges involved in that.

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \texttt{MOV        EDI, DWORD PTR [RSP + 0xC]} \\
    \midrule
    \begin{lstlisting}
        RDI = {@32[RSP + 0xC] 0 32, 0x0 32 64}
        RIP = 0x40102D
    \end{lstlisting}
    \end{tabular}
    \caption{Symbolic equations for \texttt{MOV} instruction}\label{fig:symb_equation_mov}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \texttt{ADD        QWORD PTR [RSP + 0x20], 0x9} \\
    \midrule
    \begin{lstlisting}
        @64[RSP + 0x20] = @64[RSP + 0x20] + 0x9
        zf = @64[RSP + 0x20] == 0xFFFFFFFFFFFFFFF7
        nf = (@64[RSP + 0x20] + 0x9)[63:64]
        pf = parity((@64[RSP + 0x20] + 0x9) & 0xFF)
        cf = (@64[RSP + 0x20] ^ ((@64[RSP + 0x20]
             ^ (@64[RSP + 0x20] + 0x9))
             & (@64[RSP + 0x20] ^ 0xFFFFFFFFFFFFFFF6))
             ^ (@64[RSP + 0x20] + 0x9) ^ 0x9)[63:64]
        of = ((@64[RSP + 0x20] ^ (@64[RSP + 0x20] + 0x9))
             & (@64[RSP + 0x20] ^ 0xFFFFFFFFFFFFFFF6))[63:64]
        af = (@64[RSP + 0x20] ^ (@64[RSP + 0x20] + 0x9) ^ 0x9)[4:5]
        RIP = 0x401889
    \end{lstlisting}
    \end{tabular}
    \caption[]{Symbolic equations for \texttt{ADD} instruction}\label{fig:symb_equation_add}
\end{figure}

\subsection{Verifying the Symbolic Execution Backend}\label{sec:symb_exec_backend}

As has been shown above, we rely on one translator (in this case, one which translates instructions into symbolic
equations) to be implemented correctly: It is an oracle. Fundamentally, it allows Focaccia to predict the outcome of
applying an instruction to an arbitrary program state, thus providing a truth against which an emulator's state can be
tested. The disadvantage of a reliance on a correct program is the virtual impossibility for nontrivial programs to be
correct.

To mitigate the uncertainty that is thereby introduced into a tool that is supposed to \textbf{facilitate} certainty, we
employ an online verification strategy that checks generated symbolic equations against the concrete reference state
while recording the symbolic trace. The system warns the user when it encounters an incorrectly implemented instruction
so that they shall not rely on the verifier's results regarding that particular instruction.

We do this by reusing the exact same procedure that Focaccia uses to test binary translators in the first place, but
this time use concrete states, i.e., correct states by definition, as inputs for calculating the expected state after an
instruction as well as the state against which this prediction is tested: We test whether $\sigma_i(S) = S_{i+1}$. If
the prediction made by a symbolic expression is unequal to the concrete state calculated by the processor (which, though
it could \textit{theoretically} violate an \ac{ISA}'s specification, is still the most true calculation that we are able
to obtain), then the symbolic expression generator $E_{X \rightarrow \Sigma}$ is incorrect. The pseudocode in
\figurename~\ref{fig:symb_generator_verification} illustrates this algorithm.

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Python]
        while program.is_running():
            instr = program.current_instruction
            state = program.current_state

            # Verify the previous instruction's predicted state
            if state != expected_state:
                warn_incorrect(program.prev_instruction)

            # Predict the next program state
            symb_expr = gen_symb_expr(instr)
            expected_state = symb_expr(state)

            program.step()
    \end{lstlisting}
    \end{tabular}
    \caption{Verification of the Symbolic Expression Generator}\label{fig:symb_generator_verification}
\end{figure}

\section{Tracing the Program}

As noted in the introduction to section~\ref{sec:symb_exec}, recording an entire program as one symbolic equation is
impossible. Not only do branches cause the equations to blow up exponentially, but any kind of loop will never be able
to halt at all as the termination condition can never resolve to a concrete \texttt{true} or \texttt{false} answer.

Focaccia's solution is to run the test program concretely, meaning as a native execution on the physical machine,
alongside the symbolic expression generator and to follow (or `\textit{trace}') that execution, generating symbolic
equations for each instruction executed. In this scenario, the concrete execution defines which instructions to
symbolize and in what order, while the symbolic expression generator computes those instructions' symbolic
representations. This tactic yields one specific linear \textit{symbolic trace}, which is the translation $E_{X
\rightarrow \Sigma}(P_X) = P_\Sigma = (\sigma_i)$ of a program into symbolic entities.

As already stated, this approach eliminates the most blatant shortcomings of symbolic execution, though, in doing so,
induces a dependency on a certain amount of concreteness. That concreteness contains remnants of the conceptually
eliminated initial state difference $\Delta_S$ (section~\ref{sec:comparison}). The way this difference manifests
itself is by modifying the concrete execution's path through the program, and therefore also that of the symbolic trace.

\subsection{Trace Mismatches}\label{sec:trace_mismatch}

\subsubsection{Nondeterministic Branching}

One can imagine code that behaves effectively as the following:

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Python]
        n = random()
        if n > 0.5:
            func_a()
        else:
            func_b()
    \end{lstlisting}
    \end{tabular}
    \caption{Nondeterministic branching}\label{fig:random_branching}
\end{figure}

These situations do happen, be it during iteration over environment arrays or their content, literally deciding branches
based on randomness, or generally any branching involving nondeterministic values, such as network, time, system state,
… A specific situation in which this routinely happens is the libc initialization code, where auxiliary and environment
vectors are processed. These nondeterministic mutations can cause the symbolic program trace to differ from the tested
program trace.

This is a problem for Focaccia's standard algorithm where a symbolic trace is computed from a concrete trace, the
process of which can be conceptualized as a pre-recordment because it never interacts with the tested emulator's state
directly (which is purposefully so as the emulator's correctness cannot be trusted and should therefore not partake in
\textit{truth} generation), and is afterwards applied to an emulator's program trace. If the latter contains
instructions that were never executed in the concrete execution because of different branching behaviour, the symbolic
trace will not contain information about these instructions and cannot test them.

As this problem is unresolvable by a lack of information, the verifier resorts to skipping instructions that are not
included in the symbolic trace and trying to find a point where both traces agree again. Warnings are issued to the user
when instructions are skipped. The problem is improvable by ensuring similar initial conditions for the emulator's
execution and the symbolic trace recorder (this is effort that the user has to bear), yet rarely avoidable.
Section~\ref{sec:experimental_trace_match} discusses an experimental attempt to improve this situation, though it does
suffer from major flaws.

\subsubsection{Emulation Granularity}

Another kind of trace mismatch happens if the tested emulator provides its trace on a different granularity level than
Focaccia does. A common example is emulators stepping the program forward by \textbf{basic blocks}, whereas Focaccia
always generates symbolic traces at single-instruction granularity. \figurename~\ref{fig:trace_granularity} shows a diff
view of a real-life trace granularity mismatch. On the left side of the diff, we see an emulator's basic-block-based
instruction trace (rows are addresses of executed instructions), whereas the right side shows Focaccia's
single-instruction symbolic trace generated from a concrete execution of the same executable. The first basic block
ranges from instruction \texttt{0x401032} through \texttt{0x401043}. The second basic block starts at \texttt{0x401048},
etc.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/trace_diff_view.png}
    \caption{Different Program Trace Granularities}\label{fig:trace_granularity}
\end{figure}

This mismatch is easily resolvable: In the higher-granularity symbolic trace, merge symbolic equations for all
instructions of the same basic block into a single equation, i.e., transform the higher-granularity trace into a
lower-granularity one. This will yield larger equations, though they will not blow up exponentially as basic blocks by
definition do not include branches. A simple post-processing algorithm operating on the two traces can detect basic
block boundaries reliably because one basic block never includes the same instruction multiple times.

Problematic is the case where both granularity mismatch \textbf{and} trace divergence happen in the same trace; this is
fundamentally unsolvable in a trace post-processing scenario because one cannot differentiate between excess
instructions resulting from different branching behaviour and excess instructions from higher trace granularity.

\subsection{Auxiliary Vector}\label{sec:auxv}

An interesting source of difference in branching behaviour between concrete- and emulator execution is the auxiliary
vector on Linux systems, which is "a mechanism that the kernel's ELF binary loader uses to pass certain information to
user space when a program is executed"~\cite{getauxval2024Mar}. Libc's initialization code (at least glibc as well as
musl libc~\cite{MuslLibc2024Feb}) iterates over that array to bring it into a representation that is indexable by entry
names, thereby depending its branching behaviour on the array's size. It turns out that QEMU, at least on some systems,
passes an auxiliary vector to the application that is different from the native auxiliary vector on the same system;
\figurename~\ref{fig:auxv_comparison} shows an example. While a custom method for launching both the tested emulator and
the symbolic trace recorder could in theory ensure equal environment arrays, the same is not possible for the
kernel-provided auxiliary vector.

This shows that branch-based trace mismatches are not a rare phenomenon.

\begin{figure}[htpb]
    \begin{subfigure}[t]{0.4\linewidth}
        \begin{lstlisting}
            AT_BASE : 0x790a3a4f3000
            AT_CLKTCK : 0x7ffd4e2f21b9
            AT_EGID : 0x3e8
            AT_ENTRY : 0x5d141499c050
            AT_EUID : 0x3e8
            AT_EXECFN : 0x7ffd4e2f2fec
            AT_FLAGS : 0x0
            AT_GID : 0x3e8
            AT_HWCAP : 0x64
            AT_HWCAP2 : 0x2
            AT_MINSIGSTKSZ : 0x7f0
            AT_PAGESZ : 0x1000
            AT_PHDR : 0x5d141499b040
            AT_PHENT : 0x38
            AT_PHNUM : 0xd
            AT_PLATFORM : 0xbfebfbff
            AT_RANDOM : 0x7ffd4e2f21a9
            AT_RSEQ_ALIGN : 0x20
            AT_RSEQ_FEATURE_SIZE : 0x1c
            AT_SECURE : 0x0
            AT_SYSINFO_EHDR : 0x7ffd4e349000
            AT_UID : 0x3e8
        \end{lstlisting}
    \caption{Native AUXV}
    \label{fig:native_auxv}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\linewidth}
        \begin{lstlisting}
            AT_BASE : 0x2aaaab2ac000
            AT_CLKTCK : 0x2aaaab2ab7f9
            AT_EGID : 0x3e8
            AT_ENTRY : 0x555555557050
            AT_EUID : 0x3e8
            AT_EXECFN : 0x2aaaab2abfd1
            AT_FLAGS : 0x0
            AT_GID : 0x3e8
            AT_HWCAP : 0x64
            AT_PAGESZ : 0x1000
            AT_PHDR : 0x555555556040
            AT_PHENT : 0x38
            AT_PHNUM : 0xd
            AT_PLATFORM : 0xfcbfbfd
            AT_RANDOM : 0x2aaaab2ab7e0
            AT_SECURE : 0x0
            AT_SYSINFO_EHDR : 0x2aaaab2e3000
            AT_UID : 0x3e8
        \end{lstlisting}
        \caption{QEMU's AUXV}
        \label{fig:qemu_auxv}
    \end{subfigure}
    \caption{Comparison of auxiliary vectors in native execution and QEMU}
    \label{fig:auxv_comparison}
\end{figure}

\subsection{Experimental: A Technique for Perfectly Matching Traces}\label{sec:experimental_trace_match}

TODO: Detail the custom-protocol implementation that reads the trace directly from the emulator.

\textbf{NOTE} Can be eliminated for the special case of online emulator verification. We could implement a minimal
custom protocol as a replacement for the GDB-server protocol and provide a general online verification algorithm for
emulators that implement this protocol specifically for verification with Focaccia. This algorithm would give the
highest-quality results that Focaccia is able to calculate.

\section{Recording Emulator State}

TODO: Logs, gdb-server, etc.

\section{Optimizations}

TODO: Explain the nontrivial speedups on the naive algorithm that I achieved, particularly the minimal snapshot
technology.



% APPROACH
%
%  - systematic!
%  - we don't want to write tests
%  - fuzzing has more setup work, is harder to use
%
% chapter 12 virtual machines popek & goldberg
%
% establishing V(S_j)
