\chapter{Design}\label{chapter:design}

\section{The Algorithm: An Abstract Description}\label{sec:abstract_algo}

\subsection{Local Discreteness}\label{sec:local_discreteness}

Requiring a tested translator to satisfy local discreteness is imperative to verify whether $P_X \equiv E_{X \rightarrow
Y}(P_X)$. This is because, if this requirement weren't assumed, a verifier would either have to know the result of
comparing every possible program of $X$ to every possible program of $Y$ in advance (which is obvious nonsense as both
of these sets are infinitely large) or be a sort of theoretical abstract tautology, in other words an algorithm that is
not actually implementable. This argument simultaneously shows that every implementation of a binary translator
\textbf{must necessarily} be locally discrete for the same reasons.

However, when we use the definition of program equivalence to transform the problem into $P_X(S) = [E_{X \rightarrow
Y}(P_X)](S)$, the global comparison of result states becomes possible again. While this removes the theoretical
necessity for the local discreteness assumption, it is still practically useful, if not essential. An outcome of
comparing result states of full program executions would be a statement like "The program's translation is erroneous".
This is too general of an assertion to be useful to developers. When we instead use local discreteness and check for all
intermediate states whether $S_i = S^E_i$ (where $S_i = x_{i-1}(S_{i-1})$ and $S^E_i = E_{X \rightarrow
Y}(x_{i-1})(S^E_{i-1})$ with $S_1 = S^E_1$ an arbitrary initial state), we can generate much more detailed information,
such as "Instruction $x_i$ is translated incorrectly because $S_i \neq S^E_i$". This is exactly the type of feedback
that Focaccia seeks to provide.

Additionally, it lets the algorithm recover from errors: Even if an instruction $x_i$ turns out to be implemented
incorrectly, thus producing an incorrect state $S^E_{i+1}$, that state is only incorrect from the viewpoint of program
semantics. The verifier, however, can still treat it as a valid input state to all subsequent instructions $x_{j > i}$
because the translation $E_{X \rightarrow Y}(x_j)$ is required to be \textit{locally correct} for a locally discrete
translator, that is, it must work correctly on \textbf{any} program state.

\subsection{Comparing Program States}\label{sec:comparison}

A naive verification algorithm works as follows: We run $P_X$ on one initial state $S$ and the translation $E_{X
\rightarrow Y}(P_X)$ on the same initial state, during which we record the respective intermediate states $S_i$ and
$S^E_i$. For each pair of states, test whether $S_i = S^E_i$. If this equality does \textbf{not} hold, then the
translator's implementation of $x_i$, meaning the translation $x_i \mapsto (y_j)$, is faulty.

The intuitive way of implementing the equality operator on program states ($S = S'$) is by comparing register- and
memory content. These values are what constitute the state of a program, and they are numeric values with a canonical
condition for equality. However, as section \ref{sec:intro:focaccia} indicated, comparing program states is more complex
in reality. That is because the starting states for the execution of programs are not always equal: the previous
assumption that $S_1 = S^E_1$ is not necessarily true. In fact, it is most commonly false. Possible contributing factors
(subsequently collectivized as a general \textit{difference in environment}) include:

\begin{itemize}
    \item Different initial stack pointers.
    \item Different addresses of heap allocations.
    \item Different environment- and auxiliary vectors. The latter is particularly interesting. It turns out that
        the auxiliary vector provided by QEMU, for example, routinely differs from the one provided by the operating
        system. See section \ref{sec:auxv} for more details.
\end{itemize}

Therefore, instead of comparing $P_X(S) = P_Y(S')$, which does \textbf{not} imply $P_X \equiv P_Y$ if $S \neq S'$, the
comparison must take into account the initial difference $\Delta_S = S - S'$ and establish a \textbf{state equivalence}
$P_X(S) \equiv_{\Delta_S} P_Y(S') \implies P_X \equiv P_Y$ with respect to it. This is the chief nontriviality that
Focaccia's algorithm solves.

The way we calculate this equivalence is by re-introducing information about the guest instructions $x_i$ to the
algorithm which, in order to simplify the problem, we have discarded when we transformed the central question from $x
\equiv E_{X \rightarrow Y}(x)$ to $x(S) = [E_{X \rightarrow Y}(x)](S)$. The new algorithm works as follows: Instead of
running guest program and translation in parallel and comparing their intermediate states, only run the translation
$E_{X \rightarrow Y}(P_X)$ on a start state $S^E_1$, thereby obtaining the translation's intermediate states $S^E_i$.
Then, for each $S^E_i$, use the guest instruction $x_i$ to calculate a corresponding \textbf{expected state} $S_{i+1} =
x_i(S^E_i)$. These represent truth states that would result from executing $x_i$ on $S^E_i$ if $x_i$ was implemented
correctly. Finally, compare the expected state to the actual translation state: $S^E_{i+1} = S_{i+1}$. Again, this works
because it does not matter to the verifier whether $S^E_{i+1}$ is a \textit{correct} state with regards to whole-program
semantics.

Decomposing the pre-translation program $(x_i)$ into its instructions and applying them selectively to the synthetic
test states as opposed to executing it natively on a semi-random starting state thus allows us to use the same starting
state for both the translation and the truth program at each instruction, eliminating $\Delta_S$. This enables the
desired equality comparison $x_i(S_i) = E_{X \rightarrow Y}(x_i)(S_i)$. The drawback of this approach is that we now
require an additional piece of information: We need to know $x_i$.

Not only do we need to know what every $x_i$ is, but we also demand a way of applying it individually to an arbitrary
program state which we determine, or, more precisely, which is being determined by the translation's execution. One
approach would be to generate and run a small program or piece of code that sets up the correct machine state, then runs
the instruction in question, and finally reads the resulting state back. We chose a different path: We use symbolic
execution tools to translate instructions into equations, which we then manually apply to the states we want to test.
Section~\ref{sec:symb_exec} explains how this works.

\section{Symbolic Execution}\label{sec:symb_exec}

Symbolic execution is a technique in which programs are executed abstractly with symbolic values~\cite{Steinhöfel2022}.
Certain values at any point in the program are replaced by symbols that represent multiple or all possible values.
Symbolic execution then tracks those symbols' evolution throughout the program's execution.
\lstlistingname~\ref{fig:symbexec_example_listing} shows a code example. In a usual \textit{concrete} execution,
\texttt{a} is exactly one value and subsequent code uses that value to perform calculations. If, for example, we set
\texttt{a = 4}, the program calculates the following values: \texttt{a = 4}, \texttt{b = -1}, \texttt{c = -3}. If, on
the other hand, in a symbolic execution of the same code, \texttt{a} is set to a generic symbol \texttt{a = $\alpha$},
the symbolic execution engine produces the following output: \texttt{a = $\alpha$}, \texttt{b = $\alpha$ - 5}, \texttt{c
= ((($\alpha$ - 5) \% 3) == 0) ? ($\alpha$ + 10) : (-3)}. The entire program is expressed depending on the generic
symbol $\alpha$.

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Python]
        a = ?
        b = a - 5
        if (b % 3) == 0:
            c = a + 10
        else:
            c = -3
    \end{lstlisting}
    \end{tabular}
    \caption[Symbolic execution example]{Symbolic Execution Sample Code}\label{fig:symbexec_example_listing}
\end{figure}

% TODO: Can I use citations from Steinhöfel2022 to cite every single of the following use cases or is the single
% reference to the Steinhöfel2022 chapter sufficient?

This technique is often used to reverse-engineer program inputs, to implement symbolic debugging, or to test programs
either by proving properties of the resulting equations (e.g.\ when is \texttt{c == 12}?) or by generating test cases
from the equation. We use symbolic execution tools for machine code to translate instructions into manipulable
functions/equations which we can then apply to program states via an interpreter. It turns out, that, if one executes an
instruction via a symbolic execution engine on an input state which is not only partly but entirely symbolic, the
resulting equation is a full representation of the transformation which is applied to a program state by that
instruction. This is an uncommonly seen use of symbolic execution as inputs to a program are usually symbolized
selectively, otherwise the branching complexity of full programs quickly leads to an exponential inflation of the
equations' size as well as the number of explored paths. This is not a problem if we only trace a single instruction at
any time: no symbolic branching conditions are ever propagated through multiple instructions. Overall, only one path
through the program is considered and the effort thus remains linear.

Formally, we denote this use of symbolic execution as a map $G$ from an instruction $x \in X$ to a symbolic entity
$\sigma$ which is composed from a symbolic alphabet $\Sigma$ and is itself a function of program states: $G_{X
\rightarrow \Sigma}: x \mapsto \sigma$ with $x(S) = \sigma(S)$ where $G$ is called the \textbf{symbolic expression
generator} for $X$. The perceptive reader will have noticed that this translation resembles a locally discrete binary
translator (though a generalized version whose target language is not specifically an \ac{ISA}, but an abstract symbolic
language), with the addition of a \textit{correctness condition}. This is in fact the case. Essentially, Focaccia relies
on the existence of one correct binary translator $E_{X \rightarrow \Sigma}$ which translates the guest \ac{ISA} $X$ to
a symbolic language $\Sigma$. See section~\ref{sec:symb_exec_backend} for further discussion of problems that this
approach bears.

\subsection{Obtaining an Instruction}

In the first prototype, we used a disassembly framework to load a binary and disassemble it in its entirety. This was
slow and produced many unnecessary computations (disassembling instructions that are never touched), so we started
disassembling instructions on demand, i.e., at each program counter, only read the next instruction. This works for
statically linked binaries as all information is in one file and can be loaded from there. In the third iteration, in
order to support dynamically linked programs and even \ac{JIT} compiled code, we omit loading the binary entirely and
instead read the current instruction directly from the running program's memory; this is usually a concrete execution
running alongside Focaccia.

\subsection{A Symbolic Representation}

The component concerned with everything symbolic execution related is the \textbf{symbolic expression generator}. Its
task is to translate instructions into corresponding symbolic representations which capture the instructions' semantics.
Figures~\ref{fig:symb_equation_mov} and~\ref{fig:symb_equation_add} show examples of symbolic equations generated from a
\texttt{MOV} instruction and an \texttt{ADD} instruction, respectively. Section~\ref{sec:symb_expr_impl} details how
this abstract expression generator is implemented in Focaccia and discusses the challenges involved in that.

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \texttt{MOV        EDI, DWORD PTR [RSP + 0xC]} \\
    \midrule
    \begin{lstlisting}
        RDI = {@32[RSP + 0xC] 0 32, 0x0 32 64}
        RIP = 0x40102D
    \end{lstlisting}
    \end{tabular}
    \caption{Symbolic equations for \texttt{MOV} instruction}\label{fig:symb_equation_mov}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \texttt{ADD        QWORD PTR [RSP + 0x20], 0x9} \\
    \midrule
    \begin{lstlisting}
        @64[RSP + 0x20] = @64[RSP + 0x20] + 0x9
        zf = @64[RSP + 0x20] == 0xFFFFFFFFFFFFFFF7
        nf = (@64[RSP + 0x20] + 0x9)[63:64]
        pf = parity((@64[RSP + 0x20] + 0x9) & 0xFF)
        cf = (@64[RSP + 0x20] ^ ((@64[RSP + 0x20]
             ^ (@64[RSP + 0x20] + 0x9))
             & (@64[RSP + 0x20] ^ 0xFFFFFFFFFFFFFFF6))
             ^ (@64[RSP + 0x20] + 0x9) ^ 0x9)[63:64]
        of = ((@64[RSP + 0x20] ^ (@64[RSP + 0x20] + 0x9))
             & (@64[RSP + 0x20] ^ 0xFFFFFFFFFFFFFFF6))[63:64]
        af = (@64[RSP + 0x20] ^ (@64[RSP + 0x20] + 0x9) ^ 0x9)[4:5]
        RIP = 0x401889
    \end{lstlisting}
    \end{tabular}
    \caption[]{Symbolic equations for \texttt{ADD} instruction}\label{fig:symb_equation_add}
\end{figure}

\subsection{Verifying the Symbolic Execution Backend}\label{sec:symb_exec_backend}

As has been shown above, we rely on one translator (in this case, one which translates instructions into symbolic
equations) to be implemented correctly: It is an oracle. Fundamentally, it allows Focaccia to predict the outcome of
applying an instruction to an arbitrary program state, thus providing a truth against which an emulator's state can be
tested. The disadvantage of a reliance on a correct program is the virtual impossibility for nontrivial programs to be
correct.

To mitigate the uncertainty that is thereby introduced into a tool that is supposed to \textbf{facilitate} certainty, we
employ an online verification strategy that checks generated symbolic equations against the concrete reference state
while recording the symbolic trace. The system warns the user when it encounters an incorrectly implemented instruction
so that they shall not rely on the verifier's results regarding that particular instruction.

We do this by reusing the exact same procedure that Focaccia uses to test binary translators in the first place, but
this time use concrete states, i.e., correct states by definition, as inputs for calculating the expected state after an
instruction as well as the state against which this prediction is tested: We test whether $\sigma_i(S) = S_{i+1}$. If
the prediction made by a symbolic expression is unequal to the concrete state calculated by the processor (which, though
it could \textit{theoretically} violate an \ac{ISA}'s specification, is still the most true calculation that we are able
to obtain), then the symbolic expression generator $E_{X \rightarrow \Sigma}$ is incorrect. The pseudocode in
\figurename~\ref{fig:symb_generator_verification} illustrates this algorithm.

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Python]
        while program.is_running():
            instr = program.current_instruction
            state = program.current_state

            # Verify the previous instruction's predicted state
            if state != expected_state:
                warn_incorrect(program.prev_instruction)

            # Predict the next program state
            symb_expr = gen_symb_expr(instr)
            expected_state = symb_expr(state)

            program.step()
    \end{lstlisting}
    \end{tabular}
    \caption{Verification of the Symbolic Expression Generator}\label{fig:symb_generator_verification}
\end{figure}

\section{Tracing the Program}

As noted in the introduction to section~\ref{sec:symb_exec} recording an entire program as one symbolic equation is
impossible in almost all practical cases. Not only do branches cause the equations to blow up exponentially, but any
kind of loop will never be able to halt at all as the termination condition can never resolve to a concrete
\texttt{true} or \texttt{false} answer. This limitation has been pointed out before many times, and the practical
solution is always to sacrifice accuracy (allow false positives or false negatives or both) for
performance~\cite{Baldoni2018SymbexecSurvey}.

Focaccia's solution is to run the test program concretely, meaning as a native execution on the physical machine,
alongside the symbolic expression generator and to follow (or `\textit{trace}') that execution, generating symbolic
equations for each instruction executed. In this scenario, the concrete execution defines which instructions to
symbolize and in what order, while the symbolic expression generator computes those instructions' symbolic
representations. This tactic avoids branching in the symbolic execution entirely, and yields one specific linear
\textit{symbolic trace}, which is the translation $E_{X \rightarrow \Sigma}(P_X) = P_\Sigma = (\sigma_i)$ of a specific
run of a program into symbolic entities. This approach can be classified as a type of dynamic symbolic execution in the
realm of concolic execution.

As already stated, this approach eliminates the most blatant shortcomings of symbolic execution, though, in doing so,
induces a dependency on a certain amount of concreteness. Besides sacrificing soundness (not all inputs for which the
program is faulty are actually found), that concreteness contains remnants of the conceptually eliminated initial state
difference $\Delta_S$ (see section~\ref{sec:comparison}). The way this difference manifests itself is by modifying the
concrete execution's path through the program, and therefore also that of the symbolic trace.

\subsection{Trace Mismatches}\label{sec:trace_mismatch}

\subsubsection{Nondeterministic Branching}

One can imagine code that behaves effectively as the following:

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Python]
        n = random()
        if n > 0.5:
            func_a()
        else:
            func_b()
    \end{lstlisting}
    \end{tabular}
    \caption{Nondeterministic branching}\label{fig:random_branching}
\end{figure}

These situations do happen, be it during iteration over environment arrays or their content, literally deciding branches
based on randomness, or generally any branching involving nondeterministic values, such as network, time, system state,
… A specific situation in which this routinely happens is the libc initialization code, where auxiliary and environment
vectors are processed. These nondeterministic mutations can cause the symbolic program trace to differ from the tested
program trace.

This is a problem for Focaccia's standard algorithm where a symbolic trace is computed from a concrete trace, the
process of which can be conceptualized as a pre-recordment because it never interacts with the tested emulator's state
directly (which is purposefully so as the emulator's correctness cannot be trusted and should therefore not partake in
\textit{truth} generation), and is afterwards applied to an emulator's program trace. If the latter contains
instructions that were never executed in the concrete execution because of different branching behaviour, the symbolic
trace will not contain information about these instructions and cannot test them.

As this problem is unresolvable by a lack of information, the verifier resorts to skipping instructions that are not
included in the symbolic trace and trying to find a point where both traces agree again. Warnings are issued to the user
when instructions are skipped. The problem is improvable by ensuring similar initial conditions for the emulator's
execution and the symbolic trace recorder (this is effort that the user has to bear), yet rarely avoidable.
Section~\ref{sec:experimental_trace_match} discusses an experimental attempt to improve this situation, though it does
suffer from major flaws.

\subsubsection{Emulation Granularity}

Another kind of trace mismatch happens if the tested emulator provides its trace on a different granularity level than
Focaccia does. A common example is emulators stepping the program forward by \textbf{basic blocks}, whereas Focaccia
always generates symbolic traces at single-instruction granularity. \figurename~\ref{fig:trace_granularity} shows a diff
view of a real-life trace granularity mismatch. On the left side of the diff, we see an emulator's basic-block-based
instruction trace (rows are addresses of executed instructions), whereas the right side shows Focaccia's
single-instruction symbolic trace generated from a concrete execution of the same executable. The first basic block
ranges from instruction \texttt{0x401032} through \texttt{0x401043}. The second basic block starts at \texttt{0x401048},
etc.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/trace_diff_view.png}
    \caption{Different Program Trace Granularities}\label{fig:trace_granularity}
\end{figure}

This mismatch is easily resolvable: In the higher-granularity symbolic trace, merge symbolic equations for all
instructions of the same basic block into a single equation, i.e., transform the higher-granularity trace into a
lower-granularity one. This will yield larger equations, though they will not blow up exponentially as basic blocks by
definition do not include branches. A simple post-processing algorithm operating on the two traces can detect basic
block boundaries reliably because one basic block never includes the same instruction multiple times.

Problematic is the case where both granularity mismatch \textbf{and} trace divergence happen in the same trace; this is
fundamentally unsolvable in a trace post-processing scenario because one cannot differentiate between excess
instructions resulting from different branching behaviour and excess instructions from higher trace granularity.

\subsection{Auxiliary Vector}\label{sec:auxv}

An interesting source of difference in branching behaviour between concrete- and emulator execution is the auxiliary
vector on Linux systems, which is "a mechanism that the kernel's ELF binary loader uses to pass certain information to
user space when a program is executed"~\cite{getauxval2024Mar}. Libc's initialization code (at least glibc as well as
musl libc~\cite{MuslLibc2024Feb}) iterates over that array to bring it into a representation that is indexable by entry
names, thereby depending its branching behaviour on the array's size. It turns out that QEMU, at least on some systems,
passes an auxiliary vector to the application that is different from the native auxiliary vector on the same system;
\figurename~\ref{fig:auxv_comparison} shows an example. While a custom method for launching both the tested emulator and
the symbolic trace recorder could in theory ensure equal environment arrays, the same is not possible for the
kernel-provided auxiliary vector.

This shows that branch-based trace mismatches are not a rare phenomenon.

\begin{figure}[htpb]
    \begin{subfigure}[t]{0.4\linewidth}
        \begin{lstlisting}
            AT_BASE : 0x790a3a4f3000
            AT_CLKTCK : 0x7ffd4e2f21b9
            AT_EGID : 0x3e8
            AT_ENTRY : 0x5d141499c050
            AT_EUID : 0x3e8
            AT_EXECFN : 0x7ffd4e2f2fec
            AT_FLAGS : 0x0
            AT_GID : 0x3e8
            AT_HWCAP : 0x64
            AT_HWCAP2 : 0x2
            AT_MINSIGSTKSZ : 0x7f0
            AT_PAGESZ : 0x1000
            AT_PHDR : 0x5d141499b040
            AT_PHENT : 0x38
            AT_PHNUM : 0xd
            AT_PLATFORM : 0xbfebfbff
            AT_RANDOM : 0x7ffd4e2f21a9
            AT_RSEQ_ALIGN : 0x20
            AT_RSEQ_FEATURE_SIZE : 0x1c
            AT_SECURE : 0x0
            AT_SYSINFO_EHDR : 0x7ffd4e349000
            AT_UID : 0x3e8
        \end{lstlisting}
    \caption{Native AUXV}
    \label{fig:native_auxv}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\linewidth}
        \begin{lstlisting}
            AT_BASE : 0x2aaaab2ac000
            AT_CLKTCK : 0x2aaaab2ab7f9
            AT_EGID : 0x3e8
            AT_ENTRY : 0x555555557050
            AT_EUID : 0x3e8
            AT_EXECFN : 0x2aaaab2abfd1
            AT_FLAGS : 0x0
            AT_GID : 0x3e8
            AT_HWCAP : 0x64
            AT_PAGESZ : 0x1000
            AT_PHDR : 0x555555556040
            AT_PHENT : 0x38
            AT_PHNUM : 0xd
            AT_PLATFORM : 0xfcbfbfd
            AT_RANDOM : 0x2aaaab2ab7e0
            AT_SECURE : 0x0
            AT_SYSINFO_EHDR : 0x2aaaab2e3000
            AT_UID : 0x3e8
        \end{lstlisting}
        \caption{QEMU's AUXV}
        \label{fig:qemu_auxv}
    \end{subfigure}
    \caption{Comparison of auxiliary vectors in native execution and QEMU}
    \label{fig:auxv_comparison}
\end{figure}

\subsection{Experimental: A Technique for Perfectly Matching Traces}\label{sec:experimental_trace_match}

I want to outline an experimental proposal for a technique which eliminates all trace mismatches. \textbf{If} Focaccia
had a way of interfacing with a running emulator, it could read current instructions directly from the emulator's
memory, i.e., take the emulator as a source for the program trace. However, this cannot test whether the emulator
decodes instructions correctly and fails when arbitrary memory corruptions in the code area may occur; the technique is
instead useful to verify mature emulators or hand-picked selections of test cases. It can, on the other hand, give the
best and most user friendly results that Focaccia is able to compute, free of all trace-based limitations.
Section~\ref{sec:obtaining_snapshots} further elaborates on the facets of this technique.

\section{Recording Concrete State}

The second major task, now that Focaccia can calculate truth states and compare them to emulator states, is to gather
snapshots of the tested emulator's concrete state in a representation that is usable for comparison, and do so
efficiently enough such that this part does not become a bottleneck for the entire verifier, which turned out to be
surprisingly difficult. We need one program state snapshot for each instruction that is executed.

\subsubsection{What is Program State?}

The momentary state of a program comprises the process's register- and memory content. The latter specifically being all
virtual memory pages allocated to the process in question. Everything other than these values, such as disk state,
network state, or state of peripheral hardware, is considered \textbf{input to}, not \textbf{state of} the program.

\subsubsection{Obtaining Snapshots from Emulators}\label{sec:obtaining_snapshots}

Focaccia needs a way to interface with emulators to read their state. This, of course, depends almost exclusively on
what sort of logging or debugging interfaces that are implemented by the emulator in question. The first snapshotting
mechanism that we implemented was a simple log parser specific for Arancini's log format---frankly, just because
Arancini served as the first emulator on which early prototypes of Focaccia were tested. Its log format, at that time,
was severely restricted in that Arancini exclusively wrote register values to the log, but no memory content. The latter
is always a challenge for static, non-interactive communication paths like log files as their size can easily blow out
of proportion for even moderately sized programs when the emulator dumps \textit{all} memory to disk at every
instruction. Additionally, almost all memory is never touched by each given instruction (a typical non-vector
instruction addresses 16 bytes of memory at most) and is thus not strictly required by the verifier.

The fact is that Focaccia will always have to deal with incomplete snapshots and missing information from input data.
Whether this is because of faults in the emulator, restricted communication interfaces, or incomplete format conversion
implementations: the reason is outside of our control and one cannot create information, therefore, we always have to
deal with its abscence. From this insight, we derived a key architectural principle for the verifier: It must not assume
the presence of any information, fail gracefully if key information is indeed missing, and, most importantly, scale the
quality of its calculations incrementally with more information.

But back to the matter at hand: Log files are a well decoupled solution that only requires a single parsing function to
be implemented to support a new emulator. On the other hand, dumping memory to files in a large-scale fashion is very
costly and the emulator has to \textbf{support} dumping memory to logs in the first place. Our major evaluation target,
QEMU, does not. Another more memory-efficient approach is an online debugging interface, such as the one QEMU provides
by implementing the \texttt{gdbserver} protocol. This method is less intrusive to Focaccia (in fact, it demands no
modification of Focaccia at all to interface with new emulators) but moves some of the effort to the side of the
emulator, at which the responsibility lies to implement said protocol.

\subsubsection{Minimal snapshots}\label{sec:minimal_snapshots}

A new accomplishment that is made possible when we use real-time interfaces to communicate with an emulator online is
that we can \textbf{choose} the data we want to read from the emulator. This is enables us to record \textit{minimal
snapshots} for each instruction that contain exactly the data needed to verify only that instruction. Minimal data to
verify one instruction actually encompasses information from \textbf{two} program states: The state before that
instruction is executed and the state after it. The previous state needs to contain input values to the instruction (its
\textit{operands}), the next state needs to contain the instruction's outputs (its \textit{destinations}). We can
determine both from an instruction's symbolic equations.

%Currently, the \texttt{gdbserver} protocol is supported, because this is the interface that QEMU offers. The
%implementation requires only a minimal set of supported functionality: reading register values, reading memory content,
%and stepping the emulation by single instructions.

To create a minimal snapshot $S'_i$ from a full program state $S_i$ so that it has all data required by the verifier so
that it can compare it to its respective truth state, but also calculate the next expected truth state from it, the
following information is required:

% TODO: Maybe call the emulator state S^E_i and the truth state S_i.
\begin{itemize}
    \item $\sigma_{i-1}$: The transformation that produces $S_i$. Determines the destination operands of $x_{i-1}$,
        which are required to compare $S_i$ to the expected truth state.
    \item $\sigma_i$: The transformation that acts on $S_i$. Determines the source operands of $x_i$, which are used to
        calculate the next truth state that will be compared to $S_{i+1}$.
    \item $S_{i-1}$: The previous program state on which producing instruction $x_{i-1}$ acts. We need it to record
        $S_i$ because symbolic addresses of memory accesses in $x_{i-1}$'s destination operands depend on it. Example:
        for $x_{i-1} = \texttt{mov [rsp],rax}$, we need to know the value of \texttt{rsp} in $S_{i-1}$ so that we can
        include memory at that address in $S'_i$.
    \item $S_i$: The complete current emulator state that is to be reduced to $S'_i$.
\end{itemize}

Minimal snapshots introduce a new problem: The algorithm can now establish only a \textit{lower bound} on emulator
correctness, that is, tell whether an instruction does \textbf{at least} what it is supposed to do. With entire-memory
snapshots, an upper bound, which means that an instruction does \textbf{at most} what it is supposed to do, would have
been possible to establish because its calculation does not inherently suffer from the $\Delta_S$ problem---the only
thing that needs to be tested is whether all values that should \textbf{not} have been touched by an instruction have
remained the same, no matter their concrete value. Obviously, establishing both an upper- and a lower bound would imply
certaintly that an instruction does \textbf{exactly} what it is supposed to do. With minimal snapshots and all of the
benefits that go along with them, however, that is impossible.



% APPROACH
%
%  - systematic!
%  - we don't want to write tests
%  - fuzzing has more setup work, is harder to use
%
% chapter 12 virtual machines popek & goldberg
%
% establishing V(S_j)
